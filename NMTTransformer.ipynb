{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NMTTransformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Machine Translation\n",
        "Using a Transformer model to translate from English to Spanish"
      ],
      "metadata": {
        "id": "OXHnacwji6Dy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "p_S0eyFt9vYc"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "\n",
        "import numpy as np\n",
        "import logging\n",
        "import pathlib\n",
        "import re\n",
        "import math\n",
        "import os\n",
        "import time\n",
        "import sys\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logging.getLogger('tensorflow').setLevel(logging.ERROR)  # suppress warnings"
      ],
      "metadata": {
        "id": "L_O7v0XBXrIW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "* Dataset\n",
        "* Data preprocessing\n",
        "* Positional Encoding\n",
        "* Scaled Dot Product Attention\n",
        "* Multi-head Attention\n",
        "* Point Wise NN\n",
        "* Encoder and decoder\n",
        "* Transformer Model\n",
        "* Training\n",
        "  * Learning Rate Scheduler\n",
        "  * Losses and Metrics\n",
        "  * Inference"
      ],
      "metadata": {
        "id": "22YIrMYfjVkb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "yj2P2w0Jj-ek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the file\n",
        "path_to_zip = keras.utils.get_file(\n",
        "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
        "    extract=True)\n",
        "\n",
        "path_to_file = pathlib.Path(path_to_zip).parent/'spa-eng/spa.txt'"
      ],
      "metadata": {
        "id": "Xs3QcQNukY1X"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(path):\n",
        "  text = path.read_text(encoding='utf-8')\n",
        "\n",
        "  lines = text.splitlines()\n",
        "  pairs = [line.split('\\t') for line in lines]\n",
        "\n",
        "  inp = [inp for inp, targ in pairs]\n",
        "  targ = [targ for inp, targ in pairs]\n",
        "\n",
        "  return inp, targ"
      ],
      "metadata": {
        "id": "IpOvqvPgSu3N"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "inputs, targets = load_data(path_to_file)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = len(inputs)\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "train_set = dataset.take(1200)\n",
        "\n",
        "tmp = dataset.skip(1200)\n",
        "\n",
        "val_set = tmp.take(300)\n",
        "\n",
        "test_set = tmp.skip(300)"
      ],
      "metadata": {
        "id": "xnE-4b1OS_Mq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "size = 0\n",
        "\n",
        "for batch in dataset:\n",
        "  size += 1\n",
        "\n",
        "print(f'num_batches: {size}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxhyXtjmSpnz",
        "outputId": "36e9e036-23c8-4970-bb72-368d9e8aebcc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_batches: 1858\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for X, Y in dataset.take(1):\n",
        "  print(X[:5])\n",
        "  print(Y[:5])\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxlejTKq4_-y",
        "outputId": "e859f2a9-3103-432b-8ee7-b5b53dabbd51"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'I have a sore throat and a slight fever.'\n",
            " b\"I never imagined we'd be talking about this topic today.\"\n",
            " b'Give me a telephone call when you get back.'\n",
            " b'Please correct me when I make a mistake.' b'Look before you leap.'], shape=(5,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'Me duele la garganta y tengo un poco de fiebre.'\n",
            " b'Nunca me imagin\\xc3\\xa9 que estar\\xc3\\xadamos hablando de este tema hoy.'\n",
            " b'Dame un telefonazo cuando vuelvas.'\n",
            " b'Corr\\xc3\\xadgeme cuando cometa un error, por favor.'\n",
            " b'Mira antes de saltar.'], shape=(5,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset preprocessing\n",
        "Create TextVectorization layer to tokenize and preprocess dataset"
      ],
      "metadata": {
        "id": "VS_YZrgnLWzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def standardize_text(text):\n",
        "  text = tf.strings.lower(text)\n",
        "\n",
        "  # keep space, a to z, and select punctuation\n",
        "  text = tf.strings.regex_replace(text, u'[^ a-z.?!,¿]', '')\n",
        "  \n",
        "  # add spaces around punctuation\n",
        "  text = tf.strings.regex_replace(text, u'[.?!,¿]', r' \\0 ')\n",
        "\n",
        "  # strip whitespace \n",
        "  text = tf.strings.strip(text)\n",
        "\n",
        "  # add start of sequence and end of sequence tokens\n",
        "  text = tf.strings.join(['<sos>', text, '<eos>'], separator=' ')\n",
        "\n",
        "  return text"
      ],
      "metadata": {
        "id": "50TV3n77Uzcb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "\n",
        "example_text = tf.constant('¿Hola, como estas?')\n",
        "print(example_text.numpy().decode())\n",
        "print(standardize_text(example_text).numpy().decode())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qkuo-S69JsJT",
        "outputId": "80f18850-d424-4e3d-c566-aaf7abb37810"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¿Hola, como estas?\n",
            "<sos> ¿ hola ,  como estas ? <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use an input TextVectorization layer that uses this function\n",
        "MAX_VOCAB_SIZE = 10000\n",
        "SEQUENCE_LENGTH = 100\n",
        "\n",
        "# pad sequences to SEQUENCE_LENGTH\n",
        "input_text_processor = tf.keras.layers.TextVectorization(standardize=standardize_text, max_tokens=MAX_VOCAB_SIZE, output_sequence_length=SEQUENCE_LENGTH)\n",
        "\n",
        "# and adapt to inputs\n",
        "input_text_processor.adapt(inputs)"
      ],
      "metadata": {
        "id": "VbmwB1zg8uAq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now for output\n",
        "output_text_processor = tf.keras.layers.TextVectorization(standardize=standardize_text, max_tokens=MAX_VOCAB_SIZE)\n",
        "\n",
        "# and adapt to outputs\n",
        "output_text_processor.adapt(targets)"
      ],
      "metadata": {
        "id": "UoGQUre_KwsE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "example_text = tf.constant('Hi, how are you?')\n",
        "print(example_text.numpy().decode())\n",
        "\n",
        "example_tokens = input_text_processor(example_text)\n",
        "print(example_tokens)\n",
        "\n",
        "input_vocab = np.array(input_text_processor.get_vocabulary())\n",
        "tokens = input_vocab[example_tokens.numpy()]\n",
        "' '.join(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "3WOfGNH4JM7j",
        "outputId": "31c5a704-7d30-4e67-8b68-4cb9e9fc5e3a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi, how are you?\n",
            "tf.Tensor(\n",
            "[   2 2271   19   54   28    8   11    3    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0], shape=(100,), dtype=int64)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<sos> hi , how are you ? <eos>                                                                                            '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positional Encoding"
      ],
      "metadata": {
        "id": "RS1L58vZkCot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(keras.layers.Layer):\n",
        "  '''\n",
        "    A positional encoding is a dense vector that encodes the position of a word in a sentence.\n",
        "    They capture the positional information of a word in a sentence\n",
        "    The positional encodings are added to each word's embedding\n",
        "  '''\n",
        "  def __init__(self, max_steps, max_dims, dtype=tf.float32, **kwargs):\n",
        "    super().__init__(dtype=dtype, **kwargs)\n",
        "    # must be even\n",
        "    if max_dims % 2 == 1: max_dims += 1\n",
        "    # position, dimension\n",
        "    p, i = np.meshgrid(np.arange(max_steps), np.arange(max_dims // 2))\n",
        "    pos_emb = np.empty((max_steps, max_dims))\n",
        "    # evens, sin position\n",
        "    pos_emb[:, ::2] = np.sin(p / 10000**(2 * i / max_dims)).T\n",
        "    # odds, cos position\n",
        "    pos_emb[:, 1::2] = np.cos(p / 10000**(2 * i / max_dims)).T\n",
        "\n",
        "    # add new dim for adding with embeddings\n",
        "    pos_emb = pos_emb[np.newaxis, ...]\n",
        "\n",
        "    self.positional_encoding = tf.constant(pos_emb.astype(self.dtype))\n",
        "    self.dim = max_dims\n",
        "  \n",
        "  def call(self, inputs):\n",
        "    shape = tf.shape(inputs) # (batch_size, input_seq_len, d_model)\n",
        "    return inputs + self.positional_encoding[:, :shape[-2], :]\n"
      ],
      "metadata": {
        "id": "uYiS-Ff101Lo"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scaled Dot Product Attention"
      ],
      "metadata": {
        "id": "cCI_kqJEcmbq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(query, key, value, mask):\n",
        "  matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
        "\n",
        "  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "  logits = matmul_qk / tf.math.sqrt(depth) # 4 dims\n",
        "\n",
        "  # add the mask zero out padding tokens.\n",
        "  if mask is not None:\n",
        "    logits += (mask * -1e9)\n",
        "\n",
        "  attention_weights = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "  return tf.matmul(attention_weights, value), attention_weights"
      ],
      "metadata": {
        "id": "szeDXzGxLzqj"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-head Attention"
      ],
      "metadata": {
        "id": "r4EFNdwXkKor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyMultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, d_model, num_heads):\n",
        "    super(MyMultiHeadAttention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "\n",
        "    assert d_model % self.num_heads == 0\n",
        "\n",
        "    self.depth = d_model // self.num_heads\n",
        "\n",
        "    self.wq = tf.keras.layers.Dense(d_model)\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "  def split_heads(self, x, batch_size):\n",
        "    \"\"\"Split the last dimension into (num_heads, depth).\n",
        "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "    \"\"\"\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "  def call(self, v, k, q, mask):\n",
        "    batch_size = tf.shape(q)[0]\n",
        "\n",
        "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "    scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
        "\n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "    concat_attention = tf.reshape(scaled_attention,\n",
        "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "    return output, attention_weights"
      ],
      "metadata": {
        "id": "l1f0Cp3rJl07"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp_mha = MyMultiHeadAttention(d_model=512, num_heads=8)\n",
        "y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
        "out, attn = temp_mha(y, k=y, q=y, mask=None)\n",
        "out.shape, attn.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLPXeCIW0eL-",
        "outputId": "2c389981-a5da-44b9-b421-7ce3eba96b94"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([1, 60, 512]), TensorShape([1, 8, 60, 60]))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Point Wise NN"
      ],
      "metadata": {
        "id": "y3z3WxGM6Hwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def point_wise_network(dim, dff):\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),\n",
        "      tf.keras.layers.Dense(dim)\n",
        "  ])"
      ],
      "metadata": {
        "id": "6ozHGNV76Kzo"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder and decoder"
      ],
      "metadata": {
        "id": "khiZA_6UkMc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(keras.layers.Layer):\n",
        "  '''\n",
        "  Structure:   multi-head attention -> point wise -> LayerNormalization\n",
        "               ------residual connection-------  \n",
        "  '''\n",
        "  def __init__(self, dim, n_heads, dff, dropout_rate=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.multi_attn = MyMultiHeadAttention(d_model=dim, num_heads=n_heads)\n",
        "    self.poin_wise_net = point_wise_network(dim, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "  def call(self, input, training, mask):\n",
        "    attn_output, _ = self.multi_attn(input, k=input, q=input, mask=mask)\n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "\n",
        "    out1 = self.layernorm1(input + attn_output)\n",
        "\n",
        "    pwn_output = self.poin_wise_net(out1)\n",
        "    ffn_output = self.dropout2(pwn_output, training=training)\n",
        "    out2 = self.layernorm2(out1 + pwn_output)\n",
        "\n",
        "    return out2\n",
        "\n",
        "class Decoder(keras.layers.Layer):\n",
        "  '''\n",
        "  masked multi-head attention -> LayerNorm -> multi-head attention -> Layer Norm -> point wise\n",
        "  ----------residual connection----------  ------residual connection--------- ----------residual connection-----------\n",
        "  '''\n",
        "  def __init__(self, dim, n_heads, dff, dropout_rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.multi_attn1 = MyMultiHeadAttention(d_model=dim, num_heads=n_heads)\n",
        "    self.multi_attn2 = MyMultiHeadAttention(d_model=dim, num_heads=n_heads)\n",
        "\n",
        "    self.poin_wise_net = point_wise_network(dim, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
        "    self.dropout3 = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "  def call(self, input, enc_output, training, look_ahead_mask, padding_mask):\n",
        "    attn1, attn_weights_block1 = self.multi_attn1(input, k=input, q=input, mask=look_ahead_mask)\n",
        "    attn1 = self.dropout1(attn1, training=training)\n",
        "    out1 = self.layernorm1(attn1 + input)\n",
        "\n",
        "    attn2, attn_weights_block2 = self.multi_attn2(enc_output, k=enc_output, q=out1, mask=padding_mask)\n",
        "    attn2 = self.dropout2(attn2, training=training)\n",
        "    out2 = self.layernorm2(attn2 + out1)\n",
        "\n",
        "    ffn_output = self.poin_wise_net(out2)\n",
        "    ffn_output = self.dropout3(ffn_output, training=training)\n",
        "    out3 = self.layernorm3(ffn_output + out2)\n",
        "\n",
        "    return out3, attn_weights_block1, attn_weights_block2"
      ],
      "metadata": {
        "id": "t339RilYkOo_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "sample_encoder_layer = Encoder(dim=512, n_heads=8, dff=2048)\n",
        "\n",
        "sample_encoder_layer_output = sample_encoder_layer(\n",
        "    tf.random.uniform((64, 43, 512)), False, None)\n",
        "\n",
        "sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbISYLR8MQym",
        "outputId": "0598bfaa-8295-482d-b1b7-bf9481713562"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 43, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "sample_decoder_layer = Decoder(dim=512, n_heads=8, dff=2048)\n",
        "\n",
        "sample_decoder_layer_output, _, _ = sample_decoder_layer(\n",
        "    tf.random.uniform((64, 50, 512)), sample_encoder_layer_output,\n",
        "    False, None, None)\n",
        "\n",
        "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2b3DtIKOMDV",
        "outputId": "dd3e11ad-7b2a-431c-ea91-b66c56bed2df"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 50, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Model"
      ],
      "metadata": {
        "id": "emSnNEctQVGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerModel(tf.keras.Model):\n",
        "  '''\n",
        "  run through encoder to get encoder_output and encoder_state\n",
        "  Decoder\n",
        "  '''\n",
        "  def __init__(self, \n",
        "               num_layers,\n",
        "               dim=512, \n",
        "               n_heads=8, \n",
        "               dff=2048,\n",
        "               max_steps=500, \n",
        "               vocab_size=10000, \n",
        "               ):\n",
        "    super().__init__()\n",
        "\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = keras.layers.Embedding(vocab_size, dim, mask_zero=True) # (batch_size, input_seq_len, d_model)\n",
        "    self.positional_encoding = PositionalEncoding(max_steps, dim) \n",
        "\n",
        "    self.enc_layers = [\n",
        "        Encoder(dim=dim, n_heads=n_heads, dff=dff)\n",
        "        for _ in range(num_layers)\n",
        "        ]\n",
        "\n",
        "    self.dec_layers = [\n",
        "        Decoder(dim=dim, n_heads=n_heads, dff=dff)\n",
        "        for _ in range(num_layers)\n",
        "        ]\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, training):\n",
        "    encoder_tokens = inputs[0] # (batch_size, 100)\n",
        "    decoder_tokens = inputs[1] # (batch_size, output_seq_len)\n",
        "\n",
        "    # Embed\n",
        "    encoder_embeddings = self.embedding(encoder_tokens) # (batch_size, input_seq_len, d_model)\n",
        "    decoder_embeddings = self.embedding(decoder_tokens) # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "\n",
        "    # Positionally encode\n",
        "    encoder_in = self.positional_encoding(encoder_embeddings) # (batch_size, input_seq_len, d_model)\n",
        "    decoder_in = self.positional_encoding(decoder_embeddings) # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    # Now encoder decoder stuff\n",
        "\n",
        "    # First create masks\n",
        "    padding_mask, look_ahead_mask = self.create_masks(encoder_tokens, decoder_tokens)\n",
        "\n",
        "    # Encoder\n",
        "    x = encoder_in\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x, training, padding_mask)\n",
        "    encoder_out = x # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    # Decoder\n",
        "    attention_weights = {}\n",
        "    y = decoder_in\n",
        "    for i in range(self.num_layers):\n",
        "      y, block1, block2 = self.dec_layers[i](y, encoder_out, training, look_ahead_mask, padding_mask)\n",
        "\n",
        "      attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
        "      attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
        "\n",
        "    decoder_out = y\n",
        "\n",
        "    final_output = self.final_layer(decoder_out)\n",
        "\n",
        "    return final_output, attention_weights\n",
        "\n",
        "  def create_masks(self, inp, tar):\n",
        "    padding_mask = self.create_padding_mask(inp)\n",
        "\n",
        "    # (batch_size, tar_seq_len)\n",
        "    look_ahead_mask = self.create_look_ahead_mask(tf.shape(tar)[1])\n",
        "\n",
        "    dec_target_padding_mask = self.create_padding_mask(tar)\n",
        "    look_ahead_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "\n",
        "    return padding_mask, look_ahead_mask\n",
        "\n",
        "  def create_padding_mask(self, seq):\n",
        "    # outputs '1' where a pad value of '0' is\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32) # (batch_size, seq_length)\n",
        "\n",
        "    # add extra dims to add padding to attention LOGITS\n",
        "    # (logits have 4 dims)\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "  def create_look_ahead_mask(self, size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask # (seq_len, seq_len)"
      ],
      "metadata": {
        "id": "OJmHfZ_VkUyo"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing (TRAINING)\n",
        "\n",
        "sample_transformer = TransformerModel(num_layers=2)\n",
        "# one batch\n",
        "for x, y in dataset.take(1):\n",
        "  temp_input = input_text_processor(x)\n",
        "  temp_target = output_text_processor(y)\n",
        "\n",
        "fn_out, _ = sample_transformer([temp_input, temp_target], training=True)\n",
        "\n",
        "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2giIYXqneR4z",
        "outputId": "4df4502d-ec5b-40d6-ee74-827879433782"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 17, 10000])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing (INFERENCE)\n",
        "\n",
        "sample_transformer = TransformerModel(num_layers=2)\n",
        "\n",
        "temp_input = tf.random.uniform((64, 100), dtype=tf.int64, minval=0, maxval=200)\n",
        "temp_target = tf.random.uniform((64, 36), dtype=tf.int64, minval=0, maxval=200)\n",
        "\n",
        "fn_out, _ = sample_transformer([temp_input, temp_target], training=False)\n",
        "\n",
        "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYcUal4pqJlm",
        "outputId": "a5e73826-104f-4129-da5a-ee1523ad0477"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 36, 10000])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "NogbZMeLkV4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_layers=6\n",
        "d_model=128\n",
        "num_heads=8\n",
        "dff=512\n",
        "dropout_rate = 0.1"
      ],
      "metadata": {
        "id": "sx-tZ2HzSm1f"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss and Metrics"
      ],
      "metadata": {
        "id": "sYa-Vf4Fbbmv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "metadata": {
        "id": "SDj1YFwTRg4h"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
        "                                     epsilon=1e-9)"
      ],
      "metadata": {
        "id": "P7ndRSYKRt64"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')"
      ],
      "metadata": {
        "id": "zZLQ_xVlRwNV"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "def accuracy_function(real, pred):\n",
        "  accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
        "\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  accuracies = tf.math.logical_and(mask, accuracies)\n",
        "\n",
        "  accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "  return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
      ],
      "metadata": {
        "id": "VmReJwBiSIZM"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
      ],
      "metadata": {
        "id": "7t4mPD5ASLMI"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = TransformerModel(\n",
        "    num_layers=num_layers,\n",
        "    dim=d_model,\n",
        "    n_heads=num_heads,\n",
        "    dff=dff)"
      ],
      "metadata": {
        "id": "WxfIlNy7SNSc"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = './checkpoints/train'\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print('Latest checkpoint restored!!')"
      ],
      "metadata": {
        "id": "iLU92oIdSeIC"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 50"
      ],
      "metadata": {
        "id": "IQtHXsywSrvv"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None,), dtype=tf.string),\n",
        "    tf.TensorSpec(shape=(None,), dtype=tf.string),\n",
        "]\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp, tar):\n",
        "  inp = input_text_processor(inp)\n",
        "  tar = output_text_processor(tar)\n",
        "\n",
        "  tar_inp = tar[:, :-1]\n",
        "  tar_real = tar[:, 1:]\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions, _ = transformer([inp, tar_inp],\n",
        "                                 training = True)\n",
        "    loss = loss_function(tar_real, predictions)\n",
        "\n",
        "  gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "  train_loss(loss)\n",
        "  train_accuracy(accuracy_function(tar_real, predictions))"
      ],
      "metadata": {
        "id": "9Hdtq2sQS2ab"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "\n",
        "  # inp -> english, tar -> spanish\n",
        "  for (batch, (inp, tar)) in enumerate(train_set):\n",
        "    train_step(inp, tar)\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print(f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
        "\n",
        "  if epoch % 30 == 0:\n",
        "    transformer.save(f'EngSpanModel-{epoch}')\n",
        "\n",
        "  print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "  print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "stbz1pzaS5Ue",
        "outputId": "244cdef5-53fa-44c1-d6ed-a201ca4a42b6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 9.2304 Accuracy 0.0000\n",
            "Epoch 1 Batch 100 Loss 9.0071 Accuracy 0.0803\n",
            "Epoch 1 Batch 200 Loss 8.7236 Accuracy 0.1003\n",
            "Epoch 1 Batch 300 Loss 8.3054 Accuracy 0.1074\n",
            "Epoch 1 Batch 400 Loss 7.8250 Accuracy 0.1107\n",
            "Epoch 1 Batch 500 Loss 7.4009 Accuracy 0.1218\n",
            "Epoch 1 Batch 600 Loss 7.0544 Accuracy 0.1400\n",
            "Epoch 1 Batch 700 Loss 6.7738 Accuracy 0.1567\n",
            "Epoch 1 Batch 800 Loss 6.5476 Accuracy 0.1699\n",
            "Epoch 1 Batch 900 Loss 6.3634 Accuracy 0.1806\n",
            "Epoch 1 Batch 1000 Loss 6.2031 Accuracy 0.1904\n",
            "Epoch 1 Batch 1100 Loss 6.0626 Accuracy 0.1995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as my_multi_head_attention_10_layer_call_fn, my_multi_head_attention_10_layer_call_and_return_conditional_losses, layer_normalization_15_layer_call_fn, layer_normalization_15_layer_call_and_return_conditional_losses, layer_normalization_16_layer_call_fn while saving (showing 5 of 300). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss 5.9379 Accuracy 0.2077\n",
            "Time taken for 1 epoch: 185.85 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 4.5290 Accuracy 0.3008\n",
            "Epoch 2 Batch 100 Loss 4.4491 Accuracy 0.3053\n",
            "Epoch 2 Batch 200 Loss 4.4382 Accuracy 0.3062\n",
            "Epoch 2 Batch 300 Loss 4.4050 Accuracy 0.3104\n",
            "Epoch 2 Batch 400 Loss 4.3680 Accuracy 0.3150\n",
            "Epoch 2 Batch 500 Loss 4.3353 Accuracy 0.3188\n",
            "Epoch 2 Batch 600 Loss 4.3047 Accuracy 0.3222\n",
            "Epoch 2 Batch 700 Loss 4.2764 Accuracy 0.3256\n",
            "Epoch 2 Batch 800 Loss 4.2512 Accuracy 0.3286\n",
            "Epoch 2 Batch 900 Loss 4.2276 Accuracy 0.3311\n",
            "Epoch 2 Batch 1000 Loss 4.2025 Accuracy 0.3338\n",
            "Epoch 2 Batch 1100 Loss 4.1816 Accuracy 0.3359\n",
            "Epoch 2 Loss 4.1611 Accuracy 0.3382\n",
            "Time taken for 1 epoch: 141.92 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 3.8468 Accuracy 0.3596\n",
            "Epoch 3 Batch 100 Loss 3.8382 Accuracy 0.3686\n",
            "Epoch 3 Batch 200 Loss 3.8164 Accuracy 0.3715\n",
            "Epoch 3 Batch 300 Loss 3.8038 Accuracy 0.3728\n",
            "Epoch 3 Batch 400 Loss 3.7885 Accuracy 0.3751\n",
            "Epoch 3 Batch 500 Loss 3.7767 Accuracy 0.3769\n",
            "Epoch 3 Batch 600 Loss 3.7560 Accuracy 0.3793\n",
            "Epoch 3 Batch 700 Loss 3.7391 Accuracy 0.3812\n",
            "Epoch 3 Batch 800 Loss 3.7234 Accuracy 0.3828\n",
            "Epoch 3 Batch 900 Loss 3.7091 Accuracy 0.3849\n",
            "Epoch 3 Batch 1000 Loss 3.6954 Accuracy 0.3866\n",
            "Epoch 3 Batch 1100 Loss 3.6812 Accuracy 0.3883\n",
            "Epoch 3 Loss 3.6672 Accuracy 0.3902\n",
            "Time taken for 1 epoch: 128.21 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 3.4728 Accuracy 0.3908\n",
            "Epoch 4 Batch 100 Loss 3.4172 Accuracy 0.4144\n",
            "Epoch 4 Batch 200 Loss 3.4279 Accuracy 0.4137\n",
            "Epoch 4 Batch 300 Loss 3.4154 Accuracy 0.4154\n",
            "Epoch 4 Batch 400 Loss 3.4055 Accuracy 0.4166\n",
            "Epoch 4 Batch 500 Loss 3.3957 Accuracy 0.4175\n",
            "Epoch 4 Batch 600 Loss 3.3821 Accuracy 0.4193\n",
            "Epoch 4 Batch 700 Loss 3.3731 Accuracy 0.4207\n",
            "Epoch 4 Batch 800 Loss 3.3638 Accuracy 0.4224\n",
            "Epoch 4 Batch 900 Loss 3.3531 Accuracy 0.4239\n",
            "Epoch 4 Batch 1000 Loss 3.3378 Accuracy 0.4264\n",
            "Epoch 4 Batch 1100 Loss 3.3226 Accuracy 0.4286\n",
            "Epoch 4 Loss 3.3048 Accuracy 0.4312\n",
            "Time taken for 1 epoch: 128.53 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 2.8123 Accuracy 0.4990\n",
            "Epoch 5 Batch 100 Loss 2.9901 Accuracy 0.4695\n",
            "Epoch 5 Batch 200 Loss 3.0074 Accuracy 0.4697\n",
            "Epoch 5 Batch 300 Loss 2.9891 Accuracy 0.4720\n",
            "Epoch 5 Batch 400 Loss 2.9784 Accuracy 0.4734\n",
            "Epoch 5 Batch 500 Loss 2.9661 Accuracy 0.4750\n",
            "Epoch 5 Batch 600 Loss 2.9559 Accuracy 0.4764\n",
            "Epoch 5 Batch 700 Loss 2.9421 Accuracy 0.4789\n",
            "Epoch 5 Batch 800 Loss 2.9278 Accuracy 0.4809\n",
            "Epoch 5 Batch 900 Loss 2.9143 Accuracy 0.4827\n",
            "Epoch 5 Batch 1000 Loss 2.9048 Accuracy 0.4844\n",
            "Epoch 5 Batch 1100 Loss 2.8930 Accuracy 0.4861\n",
            "Saving checkpoint for epoch 5 at ./checkpoints/train/ckpt-1\n",
            "Epoch 5 Loss 2.8806 Accuracy 0.4880\n",
            "Time taken for 1 epoch: 129.26 secs\n",
            "\n",
            "Epoch 6 Batch 0 Loss 2.5174 Accuracy 0.5427\n",
            "Epoch 6 Batch 100 Loss 2.6157 Accuracy 0.5234\n",
            "Epoch 6 Batch 200 Loss 2.5983 Accuracy 0.5258\n",
            "Epoch 6 Batch 300 Loss 2.5983 Accuracy 0.5273\n",
            "Epoch 6 Batch 400 Loss 2.5911 Accuracy 0.5290\n",
            "Epoch 6 Batch 500 Loss 2.5837 Accuracy 0.5309\n",
            "Epoch 6 Batch 600 Loss 2.5713 Accuracy 0.5330\n",
            "Epoch 6 Batch 700 Loss 2.5591 Accuracy 0.5354\n",
            "Epoch 6 Batch 800 Loss 2.5478 Accuracy 0.5378\n",
            "Epoch 6 Batch 900 Loss 2.5386 Accuracy 0.5397\n",
            "Epoch 6 Batch 1000 Loss 2.5257 Accuracy 0.5423\n",
            "Epoch 6 Batch 1100 Loss 2.5166 Accuracy 0.5442\n",
            "Epoch 6 Loss 2.5037 Accuracy 0.5467\n",
            "Time taken for 1 epoch: 128.12 secs\n",
            "\n",
            "Epoch 7 Batch 0 Loss 2.4658 Accuracy 0.5337\n",
            "Epoch 7 Batch 100 Loss 2.2840 Accuracy 0.5782\n",
            "Epoch 7 Batch 200 Loss 2.2713 Accuracy 0.5813\n",
            "Epoch 7 Batch 300 Loss 2.2578 Accuracy 0.5839\n",
            "Epoch 7 Batch 400 Loss 2.2574 Accuracy 0.5854\n",
            "Epoch 7 Batch 500 Loss 2.2542 Accuracy 0.5867\n",
            "Epoch 7 Batch 600 Loss 2.2483 Accuracy 0.5882\n",
            "Epoch 7 Batch 700 Loss 2.2410 Accuracy 0.5900\n",
            "Epoch 7 Batch 800 Loss 2.2363 Accuracy 0.5913\n",
            "Epoch 7 Batch 900 Loss 2.2307 Accuracy 0.5929\n",
            "Epoch 7 Batch 1000 Loss 2.2278 Accuracy 0.5939\n",
            "Epoch 7 Batch 1100 Loss 2.2210 Accuracy 0.5954\n",
            "Epoch 7 Loss 2.2127 Accuracy 0.5971\n",
            "Time taken for 1 epoch: 128.11 secs\n",
            "\n",
            "Epoch 8 Batch 0 Loss 1.9470 Accuracy 0.6436\n",
            "Epoch 8 Batch 100 Loss 2.0582 Accuracy 0.6187\n",
            "Epoch 8 Batch 200 Loss 2.0506 Accuracy 0.6215\n",
            "Epoch 8 Batch 300 Loss 2.0530 Accuracy 0.6228\n",
            "Epoch 8 Batch 400 Loss 2.0484 Accuracy 0.6243\n",
            "Epoch 8 Batch 500 Loss 2.0430 Accuracy 0.6257\n",
            "Epoch 8 Batch 600 Loss 2.0381 Accuracy 0.6270\n",
            "Epoch 8 Batch 700 Loss 2.0336 Accuracy 0.6278\n",
            "Epoch 8 Batch 800 Loss 2.0283 Accuracy 0.6292\n",
            "Epoch 8 Batch 900 Loss 2.0231 Accuracy 0.6304\n",
            "Epoch 8 Batch 1000 Loss 2.0188 Accuracy 0.6313\n",
            "Epoch 8 Batch 1100 Loss 2.0134 Accuracy 0.6325\n",
            "Epoch 8 Loss 2.0079 Accuracy 0.6337\n",
            "Time taken for 1 epoch: 127.53 secs\n",
            "\n",
            "Epoch 9 Batch 0 Loss 1.8947 Accuracy 0.6556\n",
            "Epoch 9 Batch 100 Loss 1.8703 Accuracy 0.6512\n",
            "Epoch 9 Batch 200 Loss 1.8766 Accuracy 0.6521\n",
            "Epoch 9 Batch 300 Loss 1.8747 Accuracy 0.6533\n",
            "Epoch 9 Batch 400 Loss 1.8679 Accuracy 0.6543\n",
            "Epoch 9 Batch 500 Loss 1.8629 Accuracy 0.6556\n",
            "Epoch 9 Batch 600 Loss 1.8590 Accuracy 0.6565\n",
            "Epoch 9 Batch 700 Loss 1.8591 Accuracy 0.6572\n",
            "Epoch 9 Batch 800 Loss 1.8582 Accuracy 0.6578\n",
            "Epoch 9 Batch 900 Loss 1.8536 Accuracy 0.6589\n",
            "Epoch 9 Batch 1000 Loss 1.8518 Accuracy 0.6593\n",
            "Epoch 9 Batch 1100 Loss 1.8513 Accuracy 0.6599\n",
            "Epoch 9 Loss 1.8519 Accuracy 0.6602\n",
            "Time taken for 1 epoch: 127.55 secs\n",
            "\n",
            "Epoch 10 Batch 0 Loss 1.6665 Accuracy 0.6848\n",
            "Epoch 10 Batch 100 Loss 1.7316 Accuracy 0.6765\n",
            "Epoch 10 Batch 200 Loss 1.7322 Accuracy 0.6759\n",
            "Epoch 10 Batch 300 Loss 1.7294 Accuracy 0.6782\n",
            "Epoch 10 Batch 400 Loss 1.7286 Accuracy 0.6788\n",
            "Epoch 10 Batch 500 Loss 1.7318 Accuracy 0.6785\n",
            "Epoch 10 Batch 600 Loss 1.7313 Accuracy 0.6788\n",
            "Epoch 10 Batch 700 Loss 1.7320 Accuracy 0.6789\n",
            "Epoch 10 Batch 800 Loss 1.7344 Accuracy 0.6790\n",
            "Epoch 10 Batch 900 Loss 1.7346 Accuracy 0.6793\n",
            "Epoch 10 Batch 1000 Loss 1.7373 Accuracy 0.6792\n",
            "Epoch 10 Batch 1100 Loss 1.7331 Accuracy 0.6802\n",
            "Saving checkpoint for epoch 10 at ./checkpoints/train/ckpt-2\n",
            "Epoch 10 Loss 1.7327 Accuracy 0.6805\n",
            "Time taken for 1 epoch: 128.03 secs\n",
            "\n",
            "Epoch 11 Batch 0 Loss 1.4340 Accuracy 0.7189\n",
            "Epoch 11 Batch 100 Loss 1.6298 Accuracy 0.6949\n",
            "Epoch 11 Batch 200 Loss 1.6337 Accuracy 0.6944\n",
            "Epoch 11 Batch 300 Loss 1.6353 Accuracy 0.6950\n",
            "Epoch 11 Batch 400 Loss 1.6342 Accuracy 0.6959\n",
            "Epoch 11 Batch 500 Loss 1.6354 Accuracy 0.6957\n",
            "Epoch 11 Batch 600 Loss 1.6402 Accuracy 0.6950\n",
            "Epoch 11 Batch 700 Loss 1.6408 Accuracy 0.6954\n",
            "Epoch 11 Batch 800 Loss 1.6400 Accuracy 0.6958\n",
            "Epoch 11 Batch 900 Loss 1.6420 Accuracy 0.6956\n",
            "Epoch 11 Batch 1000 Loss 1.6433 Accuracy 0.6952\n",
            "Epoch 11 Batch 1100 Loss 1.6438 Accuracy 0.6952\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as my_multi_head_attention_10_layer_call_fn, my_multi_head_attention_10_layer_call_and_return_conditional_losses, layer_normalization_15_layer_call_fn, layer_normalization_15_layer_call_and_return_conditional_losses, layer_normalization_16_layer_call_fn while saving (showing 5 of 300). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 Loss 1.6448 Accuracy 0.6955\n",
            "Time taken for 1 epoch: 159.62 secs\n",
            "\n",
            "Epoch 12 Batch 0 Loss 1.5381 Accuracy 0.7207\n",
            "Epoch 12 Batch 100 Loss 1.5705 Accuracy 0.7047\n",
            "Epoch 12 Batch 200 Loss 1.5721 Accuracy 0.7052\n",
            "Epoch 12 Batch 300 Loss 1.5760 Accuracy 0.7053\n",
            "Epoch 12 Batch 400 Loss 1.5749 Accuracy 0.7061\n",
            "Epoch 12 Batch 500 Loss 1.5753 Accuracy 0.7063\n",
            "Epoch 12 Batch 600 Loss 1.5729 Accuracy 0.7071\n",
            "Epoch 12 Batch 700 Loss 1.5725 Accuracy 0.7073\n",
            "Epoch 12 Batch 800 Loss 1.5705 Accuracy 0.7081\n",
            "Epoch 12 Batch 900 Loss 1.5743 Accuracy 0.7076\n",
            "Epoch 12 Batch 1000 Loss 1.5771 Accuracy 0.7074\n",
            "Epoch 12 Batch 1100 Loss 1.5784 Accuracy 0.7075\n",
            "Epoch 12 Loss 1.5812 Accuracy 0.7073\n",
            "Time taken for 1 epoch: 128.53 secs\n",
            "\n",
            "Epoch 13 Batch 0 Loss 1.4149 Accuracy 0.7373\n",
            "Epoch 13 Batch 100 Loss 1.4906 Accuracy 0.7155\n",
            "Epoch 13 Batch 200 Loss 1.5086 Accuracy 0.7142\n",
            "Epoch 13 Batch 300 Loss 1.5044 Accuracy 0.7162\n",
            "Epoch 13 Batch 400 Loss 1.5071 Accuracy 0.7165\n",
            "Epoch 13 Batch 500 Loss 1.5072 Accuracy 0.7163\n",
            "Epoch 13 Batch 600 Loss 1.5078 Accuracy 0.7165\n",
            "Epoch 13 Batch 700 Loss 1.5110 Accuracy 0.7162\n",
            "Epoch 13 Batch 800 Loss 1.5125 Accuracy 0.7161\n",
            "Epoch 13 Batch 900 Loss 1.5142 Accuracy 0.7159\n",
            "Epoch 13 Batch 1000 Loss 1.5161 Accuracy 0.7158\n",
            "Epoch 13 Batch 1100 Loss 1.5164 Accuracy 0.7160\n",
            "Epoch 13 Loss 1.5179 Accuracy 0.7160\n",
            "Time taken for 1 epoch: 127.76 secs\n",
            "\n",
            "Epoch 14 Batch 0 Loss 1.4541 Accuracy 0.7230\n",
            "Epoch 14 Batch 100 Loss 1.4205 Accuracy 0.7291\n",
            "Epoch 14 Batch 200 Loss 1.4198 Accuracy 0.7306\n",
            "Epoch 14 Batch 300 Loss 1.4336 Accuracy 0.7285\n",
            "Epoch 14 Batch 400 Loss 1.4306 Accuracy 0.7288\n",
            "Epoch 14 Batch 500 Loss 1.4367 Accuracy 0.7282\n",
            "Epoch 14 Batch 600 Loss 1.4417 Accuracy 0.7276\n",
            "Epoch 14 Batch 700 Loss 1.4426 Accuracy 0.7273\n",
            "Epoch 14 Batch 800 Loss 1.4435 Accuracy 0.7273\n",
            "Epoch 14 Batch 900 Loss 1.4444 Accuracy 0.7273\n",
            "Epoch 14 Batch 1000 Loss 1.4457 Accuracy 0.7272\n",
            "Epoch 14 Batch 1100 Loss 1.4484 Accuracy 0.7270\n",
            "Epoch 14 Loss 1.4514 Accuracy 0.7266\n",
            "Time taken for 1 epoch: 127.46 secs\n",
            "\n",
            "Epoch 15 Batch 0 Loss 1.1933 Accuracy 0.7634\n",
            "Epoch 15 Batch 100 Loss 1.3445 Accuracy 0.7398\n",
            "Epoch 15 Batch 200 Loss 1.3669 Accuracy 0.7366\n",
            "Epoch 15 Batch 300 Loss 1.3776 Accuracy 0.7356\n",
            "Epoch 15 Batch 400 Loss 1.3804 Accuracy 0.7357\n",
            "Epoch 15 Batch 500 Loss 1.3921 Accuracy 0.7346\n",
            "Epoch 15 Batch 600 Loss 1.3943 Accuracy 0.7344\n",
            "Epoch 15 Batch 700 Loss 1.3949 Accuracy 0.7343\n",
            "Epoch 15 Batch 800 Loss 1.3972 Accuracy 0.7341\n",
            "Epoch 15 Batch 900 Loss 1.3997 Accuracy 0.7337\n",
            "Epoch 15 Batch 1000 Loss 1.4025 Accuracy 0.7334\n",
            "Epoch 15 Batch 1100 Loss 1.4035 Accuracy 0.7333\n",
            "Saving checkpoint for epoch 15 at ./checkpoints/train/ckpt-3\n",
            "Epoch 15 Loss 1.4037 Accuracy 0.7334\n",
            "Time taken for 1 epoch: 127.78 secs\n",
            "\n",
            "Epoch 16 Batch 0 Loss 1.2414 Accuracy 0.7550\n",
            "Epoch 16 Batch 100 Loss 1.3167 Accuracy 0.7446\n",
            "Epoch 16 Batch 200 Loss 1.3283 Accuracy 0.7445\n",
            "Epoch 16 Batch 300 Loss 1.3309 Accuracy 0.7433\n",
            "Epoch 16 Batch 400 Loss 1.3354 Accuracy 0.7426\n",
            "Epoch 16 Batch 500 Loss 1.3363 Accuracy 0.7428\n",
            "Epoch 16 Batch 600 Loss 1.3415 Accuracy 0.7421\n",
            "Epoch 16 Batch 700 Loss 1.3469 Accuracy 0.7414\n",
            "Epoch 16 Batch 800 Loss 1.3488 Accuracy 0.7413\n",
            "Epoch 16 Batch 900 Loss 1.3528 Accuracy 0.7408\n",
            "Epoch 16 Batch 1000 Loss 1.3549 Accuracy 0.7407\n",
            "Epoch 16 Batch 1100 Loss 1.3576 Accuracy 0.7402\n",
            "Epoch 16 Loss 1.3584 Accuracy 0.7402\n",
            "Time taken for 1 epoch: 127.22 secs\n",
            "\n",
            "Epoch 17 Batch 0 Loss 1.3126 Accuracy 0.7311\n",
            "Epoch 17 Batch 100 Loss 1.2980 Accuracy 0.7456\n",
            "Epoch 17 Batch 200 Loss 1.2960 Accuracy 0.7475\n",
            "Epoch 17 Batch 300 Loss 1.2931 Accuracy 0.7484\n",
            "Epoch 17 Batch 400 Loss 1.3015 Accuracy 0.7478\n",
            "Epoch 17 Batch 500 Loss 1.3029 Accuracy 0.7478\n",
            "Epoch 17 Batch 600 Loss 1.3083 Accuracy 0.7473\n",
            "Epoch 17 Batch 700 Loss 1.3140 Accuracy 0.7466\n",
            "Epoch 17 Batch 800 Loss 1.3165 Accuracy 0.7465\n",
            "Epoch 17 Batch 900 Loss 1.3185 Accuracy 0.7465\n",
            "Epoch 17 Batch 1000 Loss 1.3223 Accuracy 0.7460\n",
            "Epoch 17 Batch 1100 Loss 1.3248 Accuracy 0.7458\n",
            "Epoch 17 Loss 1.3265 Accuracy 0.7458\n",
            "Time taken for 1 epoch: 127.74 secs\n",
            "\n",
            "Epoch 18 Batch 0 Loss 1.3255 Accuracy 0.7593\n",
            "Epoch 18 Batch 100 Loss 1.2356 Accuracy 0.7567\n",
            "Epoch 18 Batch 200 Loss 1.2481 Accuracy 0.7550\n",
            "Epoch 18 Batch 300 Loss 1.2596 Accuracy 0.7533\n",
            "Epoch 18 Batch 400 Loss 1.2627 Accuracy 0.7528\n",
            "Epoch 18 Batch 500 Loss 1.2677 Accuracy 0.7523\n",
            "Epoch 18 Batch 600 Loss 1.2696 Accuracy 0.7522\n",
            "Epoch 18 Batch 700 Loss 1.2767 Accuracy 0.7512\n",
            "Epoch 18 Batch 800 Loss 1.2786 Accuracy 0.7513\n",
            "Epoch 18 Batch 900 Loss 1.2824 Accuracy 0.7510\n",
            "Epoch 18 Batch 1000 Loss 1.2840 Accuracy 0.7511\n",
            "Epoch 18 Batch 1100 Loss 1.2844 Accuracy 0.7513\n",
            "Epoch 18 Loss 1.2892 Accuracy 0.7510\n",
            "Time taken for 1 epoch: 127.31 secs\n",
            "\n",
            "Epoch 19 Batch 0 Loss 1.4423 Accuracy 0.7260\n",
            "Epoch 19 Batch 100 Loss 1.2178 Accuracy 0.7605\n",
            "Epoch 19 Batch 200 Loss 1.2224 Accuracy 0.7584\n",
            "Epoch 19 Batch 300 Loss 1.2285 Accuracy 0.7588\n",
            "Epoch 19 Batch 400 Loss 1.2373 Accuracy 0.7579\n",
            "Epoch 19 Batch 500 Loss 1.2440 Accuracy 0.7572\n",
            "Epoch 19 Batch 600 Loss 1.2467 Accuracy 0.7570\n",
            "Epoch 19 Batch 700 Loss 1.2512 Accuracy 0.7564\n",
            "Epoch 19 Batch 800 Loss 1.2526 Accuracy 0.7563\n",
            "Epoch 19 Batch 900 Loss 1.2540 Accuracy 0.7566\n",
            "Epoch 19 Batch 1000 Loss 1.2563 Accuracy 0.7561\n",
            "Epoch 19 Batch 1100 Loss 1.2582 Accuracy 0.7559\n",
            "Epoch 19 Loss 1.2619 Accuracy 0.7556\n",
            "Time taken for 1 epoch: 127.35 secs\n",
            "\n",
            "Epoch 20 Batch 0 Loss 1.0873 Accuracy 0.7586\n",
            "Epoch 20 Batch 100 Loss 1.1920 Accuracy 0.7621\n",
            "Epoch 20 Batch 200 Loss 1.2033 Accuracy 0.7610\n",
            "Epoch 20 Batch 300 Loss 1.2037 Accuracy 0.7612\n",
            "Epoch 20 Batch 400 Loss 1.2075 Accuracy 0.7616\n",
            "Epoch 20 Batch 500 Loss 1.2137 Accuracy 0.7609\n",
            "Epoch 20 Batch 600 Loss 1.2174 Accuracy 0.7605\n",
            "Epoch 20 Batch 700 Loss 1.2236 Accuracy 0.7599\n",
            "Epoch 20 Batch 800 Loss 1.2253 Accuracy 0.7597\n",
            "Epoch 20 Batch 900 Loss 1.2278 Accuracy 0.7600\n",
            "Epoch 20 Batch 1000 Loss 1.2335 Accuracy 0.7595\n",
            "Epoch 20 Batch 1100 Loss 1.2369 Accuracy 0.7592\n",
            "Saving checkpoint for epoch 20 at ./checkpoints/train/ckpt-4\n",
            "Epoch 20 Loss 1.2405 Accuracy 0.7588\n",
            "Time taken for 1 epoch: 127.72 secs\n",
            "\n",
            "Epoch 21 Batch 0 Loss 1.2451 Accuracy 0.7452\n",
            "Epoch 21 Batch 100 Loss 1.1784 Accuracy 0.7640\n",
            "Epoch 21 Batch 200 Loss 1.1819 Accuracy 0.7638\n",
            "Epoch 21 Batch 300 Loss 1.1864 Accuracy 0.7634\n",
            "Epoch 21 Batch 400 Loss 1.1937 Accuracy 0.7629\n",
            "Epoch 21 Batch 500 Loss 1.1983 Accuracy 0.7626\n",
            "Epoch 21 Batch 600 Loss 1.2011 Accuracy 0.7627\n",
            "Epoch 21 Batch 700 Loss 1.2010 Accuracy 0.7630\n",
            "Epoch 21 Batch 800 Loss 1.2021 Accuracy 0.7633\n",
            "Epoch 21 Batch 900 Loss 1.2040 Accuracy 0.7635\n",
            "Epoch 21 Batch 1000 Loss 1.2079 Accuracy 0.7631\n",
            "Epoch 21 Batch 1100 Loss 1.2112 Accuracy 0.7630\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as my_multi_head_attention_10_layer_call_fn, my_multi_head_attention_10_layer_call_and_return_conditional_losses, layer_normalization_15_layer_call_fn, layer_normalization_15_layer_call_and_return_conditional_losses, layer_normalization_16_layer_call_fn while saving (showing 5 of 300). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21 Loss 1.2133 Accuracy 0.7627\n",
            "Time taken for 1 epoch: 159.56 secs\n",
            "\n",
            "Epoch 22 Batch 0 Loss 1.3394 Accuracy 0.7683\n",
            "Epoch 22 Batch 100 Loss 1.1460 Accuracy 0.7696\n",
            "Epoch 22 Batch 200 Loss 1.1485 Accuracy 0.7697\n",
            "Epoch 22 Batch 300 Loss 1.1593 Accuracy 0.7685\n",
            "Epoch 22 Batch 400 Loss 1.1617 Accuracy 0.7684\n",
            "Epoch 22 Batch 500 Loss 1.1685 Accuracy 0.7679\n",
            "Epoch 22 Batch 600 Loss 1.1728 Accuracy 0.7673\n",
            "Epoch 22 Batch 700 Loss 1.1761 Accuracy 0.7672\n",
            "Epoch 22 Batch 800 Loss 1.1836 Accuracy 0.7665\n",
            "Epoch 22 Batch 900 Loss 1.1892 Accuracy 0.7661\n",
            "Epoch 22 Batch 1000 Loss 1.1916 Accuracy 0.7657\n",
            "Epoch 22 Batch 1100 Loss 1.1926 Accuracy 0.7660\n",
            "Epoch 22 Loss 1.1942 Accuracy 0.7660\n",
            "Time taken for 1 epoch: 127.36 secs\n",
            "\n",
            "Epoch 23 Batch 0 Loss 0.9875 Accuracy 0.7996\n",
            "Epoch 23 Batch 100 Loss 1.1238 Accuracy 0.7735\n",
            "Epoch 23 Batch 200 Loss 1.1363 Accuracy 0.7718\n",
            "Epoch 23 Batch 300 Loss 1.1386 Accuracy 0.7721\n",
            "Epoch 23 Batch 400 Loss 1.1432 Accuracy 0.7717\n",
            "Epoch 23 Batch 500 Loss 1.1483 Accuracy 0.7711\n",
            "Epoch 23 Batch 600 Loss 1.1562 Accuracy 0.7707\n",
            "Epoch 23 Batch 700 Loss 1.1600 Accuracy 0.7704\n",
            "Epoch 23 Batch 800 Loss 1.1638 Accuracy 0.7695\n",
            "Epoch 23 Batch 900 Loss 1.1668 Accuracy 0.7695\n",
            "Epoch 23 Batch 1000 Loss 1.1698 Accuracy 0.7693\n",
            "Epoch 23 Batch 1100 Loss 1.1719 Accuracy 0.7692\n",
            "Epoch 23 Loss 1.1739 Accuracy 0.7693\n",
            "Time taken for 1 epoch: 127.74 secs\n",
            "\n",
            "Epoch 24 Batch 0 Loss 1.1143 Accuracy 0.7719\n",
            "Epoch 24 Batch 100 Loss 1.1091 Accuracy 0.7773\n",
            "Epoch 24 Batch 200 Loss 1.1189 Accuracy 0.7756\n",
            "Epoch 24 Batch 300 Loss 1.1247 Accuracy 0.7746\n",
            "Epoch 24 Batch 400 Loss 1.1273 Accuracy 0.7749\n",
            "Epoch 24 Batch 500 Loss 1.1326 Accuracy 0.7743\n",
            "Epoch 24 Batch 600 Loss 1.1375 Accuracy 0.7737\n",
            "Epoch 24 Batch 700 Loss 1.1386 Accuracy 0.7740\n",
            "Epoch 24 Batch 800 Loss 1.1418 Accuracy 0.7738\n",
            "Epoch 24 Batch 900 Loss 1.1457 Accuracy 0.7735\n",
            "Epoch 24 Batch 1000 Loss 1.1490 Accuracy 0.7733\n",
            "Epoch 24 Batch 1100 Loss 1.1534 Accuracy 0.7728\n",
            "Epoch 24 Loss 1.1568 Accuracy 0.7726\n",
            "Time taken for 1 epoch: 128.44 secs\n",
            "\n",
            "Epoch 25 Batch 0 Loss 0.9071 Accuracy 0.7965\n",
            "Epoch 25 Batch 100 Loss 1.0839 Accuracy 0.7801\n",
            "Epoch 25 Batch 200 Loss 1.0964 Accuracy 0.7789\n",
            "Epoch 25 Batch 300 Loss 1.0940 Accuracy 0.7792\n",
            "Epoch 25 Batch 400 Loss 1.1004 Accuracy 0.7788\n",
            "Epoch 25 Batch 500 Loss 1.1096 Accuracy 0.7779\n",
            "Epoch 25 Batch 600 Loss 1.1129 Accuracy 0.7774\n",
            "Epoch 25 Batch 700 Loss 1.1181 Accuracy 0.7770\n",
            "Epoch 25 Batch 800 Loss 1.1215 Accuracy 0.7769\n",
            "Epoch 25 Batch 900 Loss 1.1267 Accuracy 0.7763\n",
            "Epoch 25 Batch 1000 Loss 1.1308 Accuracy 0.7761\n",
            "Epoch 25 Batch 1100 Loss 1.1362 Accuracy 0.7756\n",
            "Saving checkpoint for epoch 25 at ./checkpoints/train/ckpt-5\n",
            "Epoch 25 Loss 1.1402 Accuracy 0.7751\n",
            "Time taken for 1 epoch: 128.06 secs\n",
            "\n",
            "Epoch 26 Batch 0 Loss 1.1587 Accuracy 0.7700\n",
            "Epoch 26 Batch 100 Loss 1.0599 Accuracy 0.7849\n",
            "Epoch 26 Batch 200 Loss 1.0782 Accuracy 0.7830\n",
            "Epoch 26 Batch 300 Loss 1.0838 Accuracy 0.7815\n",
            "Epoch 26 Batch 400 Loss 1.0862 Accuracy 0.7817\n",
            "Epoch 26 Batch 500 Loss 1.0926 Accuracy 0.7811\n",
            "Epoch 26 Batch 600 Loss 1.0997 Accuracy 0.7799\n",
            "Epoch 26 Batch 700 Loss 1.1024 Accuracy 0.7798\n",
            "Epoch 26 Batch 800 Loss 1.1063 Accuracy 0.7796\n",
            "Epoch 26 Batch 900 Loss 1.1110 Accuracy 0.7792\n",
            "Epoch 26 Batch 1000 Loss 1.1157 Accuracy 0.7786\n",
            "Epoch 26 Batch 1100 Loss 1.1205 Accuracy 0.7780\n",
            "Epoch 26 Loss 1.1239 Accuracy 0.7776\n",
            "Time taken for 1 epoch: 127.51 secs\n",
            "\n",
            "Epoch 27 Batch 0 Loss 1.2904 Accuracy 0.7505\n",
            "Epoch 27 Batch 100 Loss 1.0617 Accuracy 0.7852\n",
            "Epoch 27 Batch 200 Loss 1.0742 Accuracy 0.7829\n",
            "Epoch 27 Batch 300 Loss 1.0785 Accuracy 0.7817\n",
            "Epoch 27 Batch 400 Loss 1.0836 Accuracy 0.7810\n",
            "Epoch 27 Batch 500 Loss 1.0893 Accuracy 0.7810\n",
            "Epoch 27 Batch 600 Loss 1.0905 Accuracy 0.7806\n",
            "Epoch 27 Batch 700 Loss 1.0966 Accuracy 0.7798\n",
            "Epoch 27 Batch 800 Loss 1.1029 Accuracy 0.7792\n",
            "Epoch 27 Batch 900 Loss 1.1040 Accuracy 0.7793\n",
            "Epoch 27 Batch 1000 Loss 1.1042 Accuracy 0.7797\n",
            "Epoch 27 Batch 1100 Loss 1.1084 Accuracy 0.7794\n",
            "Epoch 27 Loss 1.1115 Accuracy 0.7790\n",
            "Time taken for 1 epoch: 127.30 secs\n",
            "\n",
            "Epoch 28 Batch 0 Loss 1.0698 Accuracy 0.7909\n",
            "Epoch 28 Batch 100 Loss 1.0421 Accuracy 0.7893\n",
            "Epoch 28 Batch 200 Loss 1.0504 Accuracy 0.7883\n",
            "Epoch 28 Batch 300 Loss 1.0585 Accuracy 0.7869\n",
            "Epoch 28 Batch 400 Loss 1.0660 Accuracy 0.7861\n",
            "Epoch 28 Batch 500 Loss 1.0680 Accuracy 0.7861\n",
            "Epoch 28 Batch 600 Loss 1.0695 Accuracy 0.7857\n",
            "Epoch 28 Batch 700 Loss 1.0798 Accuracy 0.7842\n",
            "Epoch 28 Batch 800 Loss 1.0859 Accuracy 0.7831\n",
            "Epoch 28 Batch 900 Loss 1.0891 Accuracy 0.7826\n",
            "Epoch 28 Batch 1000 Loss 1.0914 Accuracy 0.7824\n",
            "Epoch 28 Batch 1100 Loss 1.0942 Accuracy 0.7820\n",
            "Epoch 28 Loss 1.0965 Accuracy 0.7817\n",
            "Time taken for 1 epoch: 141.91 secs\n",
            "\n",
            "Epoch 29 Batch 0 Loss 0.9841 Accuracy 0.7996\n",
            "Epoch 29 Batch 100 Loss 1.0346 Accuracy 0.7910\n",
            "Epoch 29 Batch 200 Loss 1.0340 Accuracy 0.7899\n",
            "Epoch 29 Batch 300 Loss 1.0433 Accuracy 0.7884\n",
            "Epoch 29 Batch 400 Loss 1.0472 Accuracy 0.7877\n",
            "Epoch 29 Batch 500 Loss 1.0545 Accuracy 0.7868\n",
            "Epoch 29 Batch 600 Loss 1.0600 Accuracy 0.7860\n",
            "Epoch 29 Batch 700 Loss 1.0638 Accuracy 0.7858\n",
            "Epoch 29 Batch 800 Loss 1.0678 Accuracy 0.7852\n",
            "Epoch 29 Batch 900 Loss 1.0722 Accuracy 0.7846\n",
            "Epoch 29 Batch 1000 Loss 1.0762 Accuracy 0.7842\n",
            "Epoch 29 Batch 1100 Loss 1.0795 Accuracy 0.7838\n",
            "Epoch 29 Loss 1.0827 Accuracy 0.7835\n",
            "Time taken for 1 epoch: 127.44 secs\n",
            "\n",
            "Epoch 30 Batch 0 Loss 1.2068 Accuracy 0.7588\n",
            "Epoch 30 Batch 100 Loss 1.0354 Accuracy 0.7897\n",
            "Epoch 30 Batch 200 Loss 1.0305 Accuracy 0.7905\n",
            "Epoch 30 Batch 300 Loss 1.0393 Accuracy 0.7889\n",
            "Epoch 30 Batch 400 Loss 1.0462 Accuracy 0.7880\n",
            "Epoch 30 Batch 500 Loss 1.0510 Accuracy 0.7876\n",
            "Epoch 30 Batch 600 Loss 1.0578 Accuracy 0.7867\n",
            "Epoch 30 Batch 700 Loss 1.0589 Accuracy 0.7869\n",
            "Epoch 30 Batch 800 Loss 1.0615 Accuracy 0.7867\n",
            "Epoch 30 Batch 900 Loss 1.0656 Accuracy 0.7862\n",
            "Epoch 30 Batch 1000 Loss 1.0695 Accuracy 0.7857\n",
            "Epoch 30 Batch 1100 Loss 1.0718 Accuracy 0.7857\n",
            "Saving checkpoint for epoch 30 at ./checkpoints/train/ckpt-6\n",
            "Epoch 30 Loss 1.0754 Accuracy 0.7854\n",
            "Time taken for 1 epoch: 128.08 secs\n",
            "\n",
            "Epoch 31 Batch 0 Loss 0.9721 Accuracy 0.7924\n",
            "Epoch 31 Batch 100 Loss 1.0092 Accuracy 0.7946\n",
            "Epoch 31 Batch 200 Loss 1.0138 Accuracy 0.7927\n",
            "Epoch 31 Batch 300 Loss 1.0231 Accuracy 0.7911\n",
            "Epoch 31 Batch 400 Loss 1.0253 Accuracy 0.7912\n",
            "Epoch 31 Batch 500 Loss 1.0359 Accuracy 0.7900\n",
            "Epoch 31 Batch 600 Loss 1.0396 Accuracy 0.7899\n",
            "Epoch 31 Batch 700 Loss 1.0463 Accuracy 0.7892\n",
            "Epoch 31 Batch 800 Loss 1.0507 Accuracy 0.7886\n",
            "Epoch 31 Batch 900 Loss 1.0525 Accuracy 0.7885\n",
            "Epoch 31 Batch 1000 Loss 1.0574 Accuracy 0.7879\n",
            "Epoch 31 Batch 1100 Loss 1.0594 Accuracy 0.7880\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as my_multi_head_attention_10_layer_call_fn, my_multi_head_attention_10_layer_call_and_return_conditional_losses, layer_normalization_15_layer_call_fn, layer_normalization_15_layer_call_and_return_conditional_losses, layer_normalization_16_layer_call_fn while saving (showing 5 of 300). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31 Loss 1.0620 Accuracy 0.7878\n",
            "Time taken for 1 epoch: 159.92 secs\n",
            "\n",
            "Epoch 32 Batch 0 Loss 0.8537 Accuracy 0.8178\n",
            "Epoch 32 Batch 100 Loss 0.9885 Accuracy 0.7959\n",
            "Epoch 32 Batch 200 Loss 0.9952 Accuracy 0.7950\n",
            "Epoch 32 Batch 300 Loss 1.0034 Accuracy 0.7946\n",
            "Epoch 32 Batch 400 Loss 1.0130 Accuracy 0.7934\n",
            "Epoch 32 Batch 500 Loss 1.0195 Accuracy 0.7925\n",
            "Epoch 32 Batch 600 Loss 1.0254 Accuracy 0.7918\n",
            "Epoch 32 Batch 700 Loss 1.0285 Accuracy 0.7915\n",
            "Epoch 32 Batch 800 Loss 1.0313 Accuracy 0.7912\n",
            "Epoch 32 Batch 900 Loss 1.0379 Accuracy 0.7902\n",
            "Epoch 32 Batch 1000 Loss 1.0418 Accuracy 0.7899\n",
            "Epoch 32 Batch 1100 Loss 1.0450 Accuracy 0.7896\n",
            "Epoch 32 Loss 1.0503 Accuracy 0.7889\n",
            "Time taken for 1 epoch: 127.26 secs\n",
            "\n",
            "Epoch 33 Batch 0 Loss 0.9281 Accuracy 0.7985\n",
            "Epoch 33 Batch 100 Loss 0.9845 Accuracy 0.7981\n",
            "Epoch 33 Batch 200 Loss 1.0045 Accuracy 0.7932\n",
            "Epoch 33 Batch 300 Loss 1.0135 Accuracy 0.7925\n",
            "Epoch 33 Batch 400 Loss 1.0171 Accuracy 0.7922\n",
            "Epoch 33 Batch 500 Loss 1.0215 Accuracy 0.7919\n",
            "Epoch 33 Batch 600 Loss 1.0222 Accuracy 0.7922\n",
            "Epoch 33 Batch 700 Loss 1.0250 Accuracy 0.7919\n",
            "Epoch 33 Batch 800 Loss 1.0280 Accuracy 0.7917\n",
            "Epoch 33 Batch 900 Loss 1.0295 Accuracy 0.7916\n",
            "Epoch 33 Batch 1000 Loss 1.0332 Accuracy 0.7911\n",
            "Epoch 33 Batch 1100 Loss 1.0357 Accuracy 0.7910\n",
            "Epoch 33 Loss 1.0395 Accuracy 0.7906\n",
            "Time taken for 1 epoch: 127.35 secs\n",
            "\n",
            "Epoch 34 Batch 0 Loss 0.8641 Accuracy 0.8330\n",
            "Epoch 34 Batch 100 Loss 0.9652 Accuracy 0.8009\n",
            "Epoch 34 Batch 200 Loss 0.9780 Accuracy 0.7990\n",
            "Epoch 34 Batch 300 Loss 0.9918 Accuracy 0.7970\n",
            "Epoch 34 Batch 400 Loss 0.9982 Accuracy 0.7965\n",
            "Epoch 34 Batch 500 Loss 1.0005 Accuracy 0.7961\n",
            "Epoch 34 Batch 600 Loss 1.0055 Accuracy 0.7957\n",
            "Epoch 34 Batch 700 Loss 1.0086 Accuracy 0.7955\n",
            "Epoch 34 Batch 800 Loss 1.0138 Accuracy 0.7949\n",
            "Epoch 34 Batch 900 Loss 1.0208 Accuracy 0.7940\n",
            "Epoch 34 Batch 1000 Loss 1.0252 Accuracy 0.7934\n",
            "Epoch 34 Batch 1100 Loss 1.0283 Accuracy 0.7930\n",
            "Epoch 34 Loss 1.0313 Accuracy 0.7926\n",
            "Time taken for 1 epoch: 127.03 secs\n",
            "\n",
            "Epoch 35 Batch 0 Loss 0.8602 Accuracy 0.8079\n",
            "Epoch 35 Batch 100 Loss 0.9592 Accuracy 0.7999\n",
            "Epoch 35 Batch 200 Loss 0.9642 Accuracy 0.7994\n",
            "Epoch 35 Batch 300 Loss 0.9740 Accuracy 0.7986\n",
            "Epoch 35 Batch 400 Loss 0.9869 Accuracy 0.7972\n",
            "Epoch 35 Batch 500 Loss 0.9931 Accuracy 0.7968\n",
            "Epoch 35 Batch 600 Loss 0.9989 Accuracy 0.7961\n",
            "Epoch 35 Batch 700 Loss 1.0060 Accuracy 0.7953\n",
            "Epoch 35 Batch 800 Loss 1.0113 Accuracy 0.7947\n",
            "Epoch 35 Batch 900 Loss 1.0134 Accuracy 0.7944\n",
            "Epoch 35 Batch 1000 Loss 1.0173 Accuracy 0.7942\n",
            "Epoch 35 Batch 1100 Loss 1.0204 Accuracy 0.7939\n",
            "Saving checkpoint for epoch 35 at ./checkpoints/train/ckpt-7\n",
            "Epoch 35 Loss 1.0239 Accuracy 0.7934\n",
            "Time taken for 1 epoch: 127.27 secs\n",
            "\n",
            "Epoch 36 Batch 0 Loss 0.8993 Accuracy 0.7944\n",
            "Epoch 36 Batch 100 Loss 0.9599 Accuracy 0.8017\n",
            "Epoch 36 Batch 200 Loss 0.9740 Accuracy 0.7995\n",
            "Epoch 36 Batch 300 Loss 0.9755 Accuracy 0.7988\n",
            "Epoch 36 Batch 400 Loss 0.9792 Accuracy 0.7987\n",
            "Epoch 36 Batch 500 Loss 0.9840 Accuracy 0.7983\n",
            "Epoch 36 Batch 600 Loss 0.9879 Accuracy 0.7978\n",
            "Epoch 36 Batch 700 Loss 0.9923 Accuracy 0.7977\n",
            "Epoch 36 Batch 800 Loss 0.9976 Accuracy 0.7969\n",
            "Epoch 36 Batch 900 Loss 1.0017 Accuracy 0.7964\n",
            "Epoch 36 Batch 1000 Loss 1.0043 Accuracy 0.7962\n",
            "Epoch 36 Batch 1100 Loss 1.0073 Accuracy 0.7961\n",
            "Epoch 36 Loss 1.0103 Accuracy 0.7959\n",
            "Time taken for 1 epoch: 141.91 secs\n",
            "\n",
            "Epoch 37 Batch 0 Loss 0.8695 Accuracy 0.8117\n",
            "Epoch 37 Batch 100 Loss 0.9662 Accuracy 0.8021\n",
            "Epoch 37 Batch 200 Loss 0.9607 Accuracy 0.8020\n",
            "Epoch 37 Batch 300 Loss 0.9618 Accuracy 0.8016\n",
            "Epoch 37 Batch 400 Loss 0.9707 Accuracy 0.8005\n",
            "Epoch 37 Batch 500 Loss 0.9798 Accuracy 0.7993\n",
            "Epoch 37 Batch 600 Loss 0.9830 Accuracy 0.7988\n",
            "Epoch 37 Batch 700 Loss 0.9859 Accuracy 0.7987\n",
            "Epoch 37 Batch 800 Loss 0.9891 Accuracy 0.7983\n",
            "Epoch 37 Batch 900 Loss 0.9942 Accuracy 0.7977\n",
            "Epoch 37 Batch 1000 Loss 0.9993 Accuracy 0.7971\n",
            "Epoch 37 Batch 1100 Loss 1.0025 Accuracy 0.7967\n",
            "Epoch 37 Loss 1.0035 Accuracy 0.7967\n",
            "Time taken for 1 epoch: 127.43 secs\n",
            "\n",
            "Epoch 38 Batch 0 Loss 0.8753 Accuracy 0.8125\n",
            "Epoch 38 Batch 100 Loss 0.9336 Accuracy 0.8041\n",
            "Epoch 38 Batch 200 Loss 0.9363 Accuracy 0.8042\n",
            "Epoch 38 Batch 300 Loss 0.9471 Accuracy 0.8031\n",
            "Epoch 38 Batch 400 Loss 0.9581 Accuracy 0.8022\n",
            "Epoch 38 Batch 500 Loss 0.9649 Accuracy 0.8011\n",
            "Epoch 38 Batch 600 Loss 0.9708 Accuracy 0.8005\n",
            "Epoch 38 Batch 700 Loss 0.9768 Accuracy 0.7996\n",
            "Epoch 38 Batch 800 Loss 0.9829 Accuracy 0.7990\n",
            "Epoch 38 Batch 900 Loss 0.9878 Accuracy 0.7986\n",
            "Epoch 38 Batch 1000 Loss 0.9907 Accuracy 0.7983\n",
            "Epoch 38 Batch 1100 Loss 0.9934 Accuracy 0.7981\n",
            "Epoch 38 Loss 0.9960 Accuracy 0.7979\n",
            "Time taken for 1 epoch: 127.41 secs\n",
            "\n",
            "Epoch 39 Batch 0 Loss 0.8463 Accuracy 0.8311\n",
            "Epoch 39 Batch 100 Loss 0.9163 Accuracy 0.8108\n",
            "Epoch 39 Batch 200 Loss 0.9360 Accuracy 0.8059\n",
            "Epoch 39 Batch 300 Loss 0.9473 Accuracy 0.8040\n",
            "Epoch 39 Batch 400 Loss 0.9558 Accuracy 0.8028\n",
            "Epoch 39 Batch 500 Loss 0.9625 Accuracy 0.8017\n",
            "Epoch 39 Batch 600 Loss 0.9663 Accuracy 0.8014\n",
            "Epoch 39 Batch 700 Loss 0.9679 Accuracy 0.8018\n",
            "Epoch 39 Batch 800 Loss 0.9712 Accuracy 0.8015\n",
            "Epoch 39 Batch 900 Loss 0.9748 Accuracy 0.8015\n",
            "Epoch 39 Batch 1000 Loss 0.9804 Accuracy 0.8008\n",
            "Epoch 39 Batch 1100 Loss 0.9831 Accuracy 0.8003\n",
            "Epoch 39 Loss 0.9864 Accuracy 0.8000\n",
            "Time taken for 1 epoch: 126.88 secs\n",
            "\n",
            "Epoch 40 Batch 0 Loss 0.7925 Accuracy 0.8440\n",
            "Epoch 40 Batch 100 Loss 0.9234 Accuracy 0.8062\n",
            "Epoch 40 Batch 200 Loss 0.9245 Accuracy 0.8075\n",
            "Epoch 40 Batch 300 Loss 0.9371 Accuracy 0.8053\n",
            "Epoch 40 Batch 400 Loss 0.9486 Accuracy 0.8034\n",
            "Epoch 40 Batch 500 Loss 0.9504 Accuracy 0.8032\n",
            "Epoch 40 Batch 600 Loss 0.9560 Accuracy 0.8025\n",
            "Epoch 40 Batch 700 Loss 0.9597 Accuracy 0.8024\n",
            "Epoch 40 Batch 800 Loss 0.9639 Accuracy 0.8020\n",
            "Epoch 40 Batch 900 Loss 0.9673 Accuracy 0.8015\n",
            "Epoch 40 Batch 1000 Loss 0.9706 Accuracy 0.8014\n",
            "Epoch 40 Batch 1100 Loss 0.9724 Accuracy 0.8014\n",
            "Saving checkpoint for epoch 40 at ./checkpoints/train/ckpt-8\n",
            "Epoch 40 Loss 0.9770 Accuracy 0.8009\n",
            "Time taken for 1 epoch: 127.83 secs\n",
            "\n",
            "Epoch 41 Batch 0 Loss 0.8593 Accuracy 0.8296\n",
            "Epoch 41 Batch 100 Loss 0.9117 Accuracy 0.8079\n",
            "Epoch 41 Batch 200 Loss 0.9241 Accuracy 0.8067\n",
            "Epoch 41 Batch 300 Loss 0.9350 Accuracy 0.8059\n",
            "Epoch 41 Batch 400 Loss 0.9387 Accuracy 0.8058\n",
            "Epoch 41 Batch 500 Loss 0.9427 Accuracy 0.8052\n",
            "Epoch 41 Batch 600 Loss 0.9454 Accuracy 0.8050\n",
            "Epoch 41 Batch 700 Loss 0.9531 Accuracy 0.8040\n",
            "Epoch 41 Batch 800 Loss 0.9583 Accuracy 0.8034\n",
            "Epoch 41 Batch 900 Loss 0.9606 Accuracy 0.8033\n",
            "Epoch 41 Batch 1000 Loss 0.9634 Accuracy 0.8030\n",
            "Epoch 41 Batch 1100 Loss 0.9684 Accuracy 0.8024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as my_multi_head_attention_10_layer_call_fn, my_multi_head_attention_10_layer_call_and_return_conditional_losses, layer_normalization_15_layer_call_fn, layer_normalization_15_layer_call_and_return_conditional_losses, layer_normalization_16_layer_call_fn while saving (showing 5 of 300). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 41 Loss 0.9704 Accuracy 0.8023\n",
            "Time taken for 1 epoch: 158.80 secs\n",
            "\n",
            "Epoch 42 Batch 0 Loss 0.9128 Accuracy 0.8177\n",
            "Epoch 42 Batch 100 Loss 0.9114 Accuracy 0.8096\n",
            "Epoch 42 Batch 200 Loss 0.9150 Accuracy 0.8085\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-e7b065c31cb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;31m# inp -> english, tar -> spanish\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the model for future use"
      ],
      "metadata": {
        "id": "iKZUWjNzbFAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.save('EngSpanModel')"
      ],
      "metadata": {
        "id": "Pm9DDisxbEUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r EngSpanModel.zip EngSpanModel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thpTNDkLOxxC",
        "outputId": "5d11a555-6f9c-495b-d88e-9c32cd423066"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: EngSpanModel-40/ (stored 0%)\n",
            "  adding: EngSpanModel-40/assets/ (stored 0%)\n",
            "  adding: EngSpanModel-40/keras_metadata.pb (deflated 96%)\n",
            "  adding: EngSpanModel-40/saved_model.pb (deflated 89%)\n",
            "  adding: EngSpanModel-40/variables/ (stored 0%)\n",
            "  adding: EngSpanModel-40/variables/variables.index (deflated 78%)\n",
            "  adding: EngSpanModel-40/variables/variables.data-00000-of-00001 (deflated 8%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "Y9RqTtGh73Fd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip EngSpanModel.zip\n",
        "\n",
        "loaded_model = tf.keras.models.load_model('EngSpanModel-40')"
      ],
      "metadata": {
        "id": "TZ5xOT_S72LK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2812b53f-f7c2-4208-cd65-6b190bd09b9b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  EngSpanModel.zip\n",
            "   creating: EngSpanModel-40/\n",
            "   creating: EngSpanModel-40/assets/\n",
            "  inflating: EngSpanModel-40/keras_metadata.pb  \n",
            "  inflating: EngSpanModel-40/saved_model.pb  \n",
            "   creating: EngSpanModel-40/variables/\n",
            "  inflating: EngSpanModel-40/variables/variables.index  \n",
            "  inflating: EngSpanModel-40/variables/variables.data-00000-of-00001  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model.save_weights(\"weights.h5\")"
      ],
      "metadata": {
        "id": "7jtI34O64PsG"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Model Weights**"
      ],
      "metadata": {
        "id": "qZq6yGUP4_kE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_layers=6\n",
        "d_model=128\n",
        "num_heads=8\n",
        "dff=512\n",
        "dropout_rate = 0.1\n",
        "\n",
        "transformer = TransformerModel(\n",
        "    num_layers=num_layers,\n",
        "    dim=d_model,\n",
        "    n_heads=num_heads,\n",
        "    dff=dff)\n",
        "\n",
        "temp_input = tf.random.uniform((64, 100), dtype=tf.int64, minval=0, maxval=200)\n",
        "temp_target = tf.random.uniform((64, 36), dtype=tf.int64, minval=0, maxval=200)\n",
        "\n",
        "_, _ = transformer([temp_input, temp_target], training=False)\n",
        "\n",
        "transformer.load_weights(\"weights.h5\")"
      ],
      "metadata": {
        "id": "b_k08CJY4ahp"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Translator(tf.Module):\n",
        "  def __init__(self, input_text_processor, output_text_processor, transformer, max_length=100):\n",
        "    self.input_text_processor = input_text_processor\n",
        "    self.output_text_processor = output_text_processor\n",
        "    self.transformer = transformer\n",
        "    self.max_length = max_length\n",
        "\n",
        "  def __call__(self, input_text, output_vocab):\n",
        "    if len(input_text.shape) == 0:\n",
        "      input_text = input_text[tf.newaxis]\n",
        "\n",
        "\n",
        "    input_tokens = self.input_text_processor(input_text)\n",
        "\n",
        "    encoder_input = input_tokens\n",
        "\n",
        "    start_end = self.output_text_processor([\"\"])[0]\n",
        "    start = start_end[0][tf.newaxis]\n",
        "    end = start_end[1][tf.newaxis]\n",
        "\n",
        "    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
        "    output_array = output_array.write(0, start)\n",
        "\n",
        "    for i in tf.range(self.max_length):\n",
        "      output = tf.transpose(output_array.stack())\n",
        "\n",
        "      predictions, _ = self.transformer([encoder_input, output], training=False)\n",
        "\n",
        "      # select the last token from the seq_len dimension\n",
        "      predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "      predicted_id = tf.argmax(predictions, axis=-1)\n",
        "\n",
        "      output_array = output_array.write(i+1, predicted_id[0])\n",
        "\n",
        "      if tf.equal(predicted_id, end):\n",
        "        break\n",
        "    \n",
        "    output = tf.transpose(output_array.stack()) # (1, tokens)\n",
        "    \n",
        "\n",
        "    tokens = tf.TensorArray(dtype=tf.string, size=0, dynamic_size=True)\n",
        "\n",
        "    for i in tf.range(tf.shape(output[0])[0]):\n",
        "      val = output_vocab[output[0][i]]\n",
        "      tokens = tokens.write(i, val)\n",
        "\n",
        "    predicted_words = tf.transpose(tokens.stack())\n",
        "\n",
        "    return predicted_words\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "r5mQdHsWPq5N"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translator = Translator(input_text_processor, output_text_processor, transformer)\n",
        "\n",
        "sentence = \"this is a problem we have to solve\"\n",
        "\n",
        "\n",
        "output_vocab = tf.constant(output_text_processor.get_vocabulary())\n",
        "\n",
        "\n",
        "prediction = translator(tf.constant(sentence), output_vocab)\n",
        "\n",
        "print(prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAJlIDc4_351",
        "outputId": "a66cc7d0-3c8a-48fd-be89-a4f7fd2c9735"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'<sos>' b'este' b'es' b'un' b'problema' b'que' b'tenemos' b'que'\n",
            " b'resolver' b'.' b'<eos>'], shape=(11,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Export"
      ],
      "metadata": {
        "id": "2sOgMmWAXMOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ExportTranslator(tf.Module):\n",
        "    '''\n",
        "    output_vocab needs to be passed in from outside\n",
        "    because its get_vocabulary method cannot be converted\n",
        "    to a Tensorflow Graph\n",
        "  '''\n",
        "  def __init__(self, translator, ouput_vocab):\n",
        "    self.translator = translator\n",
        "    self.output_vocab = ouput_vocab\n",
        "\n",
        "  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\n",
        "  def __call__(self, sentence):\n",
        "    return self.translator(sentence, self.output_vocab)"
      ],
      "metadata": {
        "id": "OgnsR9L6XGeh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "export_translator = ExportTranslator(translator, output_vocab)"
      ],
      "metadata": {
        "id": "0sG7ZZ4gXNoP"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "export_translator(tf.constant(\"this is a problem we have to solve\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPeGHhvRXTG7",
        "outputId": "ceae8c5c-f400-42aa-8e88-a7f2eef13fcb"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(11,), dtype=string, numpy=\n",
              "array([b'<sos>', b'este', b'es', b'un', b'problema', b'que', b'tenemos',\n",
              "       b'que', b'resolver', b'.', b'<eos>'], dtype=object)>"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(export_translator, export_dir='translator')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Cu1ZezCXWjR",
        "outputId": "47243756-857b-45e6-903f-484de95bdd1e"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as my_multi_head_attention_4_layer_call_fn, my_multi_head_attention_4_layer_call_and_return_conditional_losses, layer_normalization_5_layer_call_fn, layer_normalization_5_layer_call_and_return_conditional_losses, layer_normalization_6_layer_call_fn while saving (showing 5 of 300). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reloaded = tf.saved_model.load('translator')"
      ],
      "metadata": {
        "id": "79YHxH5ZX4P2"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reloaded(tf.constant(\"this is a problem we have to solve\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hajvi4KcX7ph",
        "outputId": "ad636fd5-0354-415b-f406-2d8961cd5980"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(11,), dtype=string, numpy=\n",
              "array([b'<sos>', b'este', b'es', b'un', b'problema', b'que', b'tenemos',\n",
              "       b'que', b'resolver', b'.', b'<eos>'], dtype=object)>"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "66WsX_baG_EV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}